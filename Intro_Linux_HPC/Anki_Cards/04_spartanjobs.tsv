Is it possible to submit a job to Slurm without any directives?	Yes, it is possible to submit without directives, in which case Slurm will use default directives (e.g., cascade or sapphire partition, one core, etc).
What is the convention to express a potential directive as a comment?`A potential directive can be expressed as a comment by separating the # symbol from the SBATCH phrase.
What is the default wall-time for a job?	The default walltime for a job is ten minutes. Except for tests, this is one default that ought to be changed.
What is the directive for changing a job's walltime?	The directive to change a job's walltime is #SBATCH -t $hours:$minutes` or `#SBACTH --time= $hours:$minutes`.
What is the directive to explicitly run a job on a single core? Is it necessary?	The directove to run a job on a single core is `#SBATCH --ntasks=1`. It is not necessary to specify this as the scheduler will assume a single-core job by default.
What is the directive to name a job?	The directive to name a job is `#SBATC --job-name=$name`.
What are the directives to be notified of a job's progress?	Notifications from the scheduler involves two directives; the first is to specify an email address e.g., `SBATCH --mail-user=youremailaddress@unimelb.edu.au` and the second for specific events. The directive `SBATCH --mail-type=ALL` will send an email when a job starts, when it ends, or if it aborts early.
MATLAB is a popular environment and high-level programming language for computational tasks. How does one submit .m files in batch mode to a scheduler? Why is it done this way?	MATLAB, in an interactive mode, typically comes with an extensive graphic environment. This is not desired in a batch mode and should be turned off. The appropriate command is therefore: `matlab -nodesktop -nodisplay -nosplash< $file.m`
What is the difference between a multi-task job and a multi-threaded job?	A multi-task job will have indepdendent tasks and can use software libraries (e.g., MPI) that allow the task to scale beyond a single compute node. A multi-threaded job is still based around one task but can have additional CPUs allocated to execution threads. It cannot scale beyond a single node.
What is the directive for a multi-threaded application?	The directive for a multi-threaded application is `#SNATCH --cpus-per-task=$X`.
What is the directive in an application to make it run with multiple threads, assuming that the resources have been requested as scheduler directives?	There is no universal application directive. It will vary on the application in question. Some will require the number of threads to be explicitly stated, others will try to use what resources are available.
Can directives be used to make an application run in parallel?	Slurm directives simply allocate resources. An application must be written in such a manner that makes use of parallelisation, whether muli-task or multi-threaded. Simply adding more tasks or cpus will not automatically make an application run in parallel, but will allow for the application to use those resources if it is designed to do so.
What happens if one allocates 8 cpus to a single-threaded application?	The application will still run using 1 core. The other 7 cores are allocated to job, but don't do anything.
What is a multi-node job compared to a multi-threaded job?	A multinode job spans more than one compute node unlike multi-threaded jobs and is designed for distributed memory jobs (e.g., built with MPI) that make use of multiple independent tasks in communication.
Consider a job with the directives: `#SBATCH --nodes=2` and `#SBATCH --ntasks-per-node=8`. How many cores is it using? Why would a job be structured this way?	A job with these directives would use up 16 cores in total, 8 for each node. A job submission like this would probably occur when there are only 8 free cores available on two compute nodes; in most cases a single node with 16 cores would be preferable for performance.
Consider a job with the directive: `#SBATCH --ntasks=16`. How many compute nodes will this run on?	The answer is unknown, but 16 or less. It could run on 16 nodes, with 1 core each, or it could run with all 16 cores on a single node, or any other combination. Slurm does its best to ensure compute nodes are contiguous when cores span more than one node as that is more efficient.
Where are example job scripts stored on Spartan?	Example job scripts are located in the `/apps/examples` directory; there are examples for over 160 directories located there.
What is the Spartan job script generator?	The job script generator is a step-by-step script generator that goes through the various Slurm directives with explanatory notes. It is located at `https://dashboard.hpc.unimelb.edu.au/forms/script_generator/`.
What is a job array?	A job array is a Slurm script with the addition of an index variable to provide multiple job submissions, as subjobs, simultaneously. Each subjob is treated like an individual job.
What is a common use case for job arrays?	Job arrays are often used when the same script is run over dozens or hundreds of independent datasets. 
Why is a job array efficient? A job array launches jobs simultaneously, compared to a loop which will carry out tasks simultaneously; assuming resource availability a job arrays performance improvement is ideally up to the size of the array (minus overhead on the scheduler), compared to a loop. For example, a job array with a thousand subjobs will be up to a thousand times quicker to process that a singe job that iterates over a thousand datasets.
What is the directive for a job array and what is the shell variable?	The directive is a resource request that establishes the size of the srray (e.g., `#SBATCH ­­array=1­-10`. The shell variable invokes a value established by the diretcive (e.g., `${Slurm_ARRAY_TASK_ID}`).
How can job array ranges be specified?	Job array ranges can be specified as a range (e.g., `#SBATCH --array=0-31`), as comma-separated values (e.g., `#SBATCH --array=1,2,5,19,27`), or with a steping sixe (e.g., `#SBATCH --array=1-7:2`).
Why must the job array directive and shell variable be represented as an integer?	Because a job array represents a subjob which is identified as an index, the reference identifier must also be an integer.
If the files for a job array do not include in their integer reference, how can they be added to an array?	To make a filename suitable for an array, the file or files must be renamed to include an integer. Another option is to parse the filenames in a directory through sed with line numbers submitted to the array.
Do job arrays operate on datasets or programs?	The array script establishes resources and provides a variable that can be used to on either datasets or programs.
How does one cancel a subjob in an array?	The job ID of an array takes the form of JobID_subjobid. This means that a subjob can be cancelled without affecting the rest of the job.
What is a job dependency?	A job dependency is when the launch of one job is dependent on the state of another job.
What is the most common use for a job dependency?	Job dependencies are commonly used when the output of Job A is the input for Job B.
What are some of the dependent conditions that can be placed on a job?	Some dependencies include: `after:jobid` job can begin after the specified jobs have started, `afterany:jobid`, job can begin after the specified jobs have terminated, `afternotok:jobid`job can begin after the specified jobs have failed, `afterok:jobid` job can begin after the specified jobs have run to completion with an exit code of zero, `singleton` job can begin execution after all previously launched jobs with the same name and user have ended. 
What is the syntax for launching a job with dependencies? 	A job dependency submission takes the general form of: `sbatch --dependency=afterok:jobid1:jobid2 job3.slurm`
How can the job ID for a dependency be automatically invoked?	Because adding the job ID at launch can be onerous, the use of a shell variables can capture the ID and submit it. e.g., `FIRST=$(sbatch myjob1.slurm); echo $FIRST; SUB1=$(echo ${FIRST##* })`.
Can job arrays and job dependecies be combined?	A job array can be submitted with dependent conditions, a job dependency can launch an job with an array. The two directiev features can be combined.
What does an interactive job do?	An interactive job launches a job with specified resources which the user can access and interact with in a real-time manner.
What are some of the uses of an interactive job?	An interactive job provides the opportunity for testing and response, as well as carrying out computationally intensive tasks that shouldn't run on login node.
What is the command to launch an ineractive job? How are resources specified?	The command `sinteractive` is used to launch the job, with resources specified as part of the command (otherwise it uses defaults). e.g., `sinteractive --nodes=1 --ntasks-per-node=2 --time=0:15:0` would launch a job for 15 minute on a single node with two cores.
What constraints are on interactive jobs? Where is the job run?	An interactive job runs on the interactive partition, which has a smaller number of nodes and a shorter maximum walltime (see with `sinfo`). Interactive jobs run directly on the compute node.
Why would an interactive job not seem to launch?	Interactive jobs are still queued, they have a jobID, and are still subject to resource availability and all other criteria for job launch. An interactive job that requests resources when the partition is busy will not run.
How does one cnnduct graphic visualisation through an interactive job?	 By adding a flag to the Spartan login (e.g., `ssh -X spartan`) and to the sinteractive command (e.g., `sinteractive --x11=first`), graphical applications can be forwarded to the user's local system. An X-windows client is required on the local system.
How can one do graphics forwarding from a GPU node?	A command that specifies the partition needs to be added as a directive, along with the GPU resource requests (e.g., `sinteractive --x11=first --partition=gpu-a100-short --gres=gpu:1`)
What are X-Windows cilents for various operating systems?	Linux systems will typically have an X-windows client. Macs can use XQuartz. For MS-Windows, MobaXterm is recommended.
What is Open OnDemand?	Open OnDemand is a web-based portal to HPC resources that allows for graphical jobs on the interactive partition, with the graphics translation conducted through the local browser.
How is Open OnDemand accessed?	Open OnDemand is accessed like Spartan (username and password) from the following URL. 	`https://spartan-ood.hpc.unimelb.edu.au/`; it is treated like an interactive jobs in terms of queueing and the partition it runs on.
What are some graphical applications that Open OnDemand has?	Open OnDemand allows for GUIs for MATLAB, Paraview, Relion, Code Server, Jupyter Notebooks, RStudio Server, TensorBoard, and Cryosparc.
What are job steps? How are resources determined?	A job can be submitted with multiple job steps, that is individual tasks that are carried out in sequence. Apart from convenience, it also allows for separate serial, multi-threaded, and multi-node applications to run as part of the same job script. When calculating resources, an aggregated value is required for walltime, and a maximum value for nodes and cores.
Are multi-step jobs efficient? What is an alternative?	Multiple job steps are efficient if a task uses less resources that a separate job launch (usually a small step of a couple a minutes). Otherwise, for better efficiency and control, a job dependency should be used.
What is scheduler backfilling?	When more resource-intensive jobs are running it is possible that gaps ends up in the resource allocation. To fill these gaps and make best use of resoures, low-resource jobs can fill those slots.
What is a common memory-based error or cause of a job to crash? Why does this occur?	An out-of-memory (OOM) error is a common cause of a job to crash. It occurs because some applications have large datasets that run in memory (for example) and the default scheduler allocation is insufficient.
How is the default memory of a job determined? How is this changed?	The scheduler will set memory equal to the total amount on a compute node divided by the number of cores requested. This can be altered with the with the `#SBATCH --mem=[mem][M|G|T]` directive (entire job) or `#SBATCH --mem-per-cpu=[mem][M|G|T]` (per core) directive. 
When jobs have a lot of data movement, where should the job launch from?	A job with a lot of movement of data from disk to compute should run from directories that are closest to the compute or are on a faster network. The home/ directory is slowest, project/ and scratch/ are faster, and local disk /var/local/tmp is the fasted (as it is the disk on the compute node), but the data must be explicitly be copied back to a directory shared over the entire system before the job ends.
